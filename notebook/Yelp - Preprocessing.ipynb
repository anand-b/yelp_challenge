{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Yelp Dataset\n",
    "\n",
    "##Phase I - Pre-processing the data:\n",
    "\n",
    "###Objectives in this phase:\n",
    "The main objective of this phase is **exploratory data analysis**. We do it by analysing each attribute and,\n",
    "1. Look at the distribution (in case of numerical attribute)\n",
    "2. Standardize the data (in case of numerical attribute)\n",
    "3. Convert categorical data to numerical data\n",
    "4. Identify the output variable(s)\n",
    "5. Feature Engineering (removing redundant data features, finding dependent features)\n",
    "    - PCA - finding independent and uncorrelated features that have significant variance\n",
    "    - summarizing or clustering data with not much variance\n",
    "6. Visualizing the data\n",
    "    - histograms\n",
    "    - scatter plot of output vs each attribute\n",
    "7. Identify the features among the attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Splitting the dataset into small chunks:\n",
    "First, we split the large dataset into small chunks as separate files. Primarily, user.json and review.json are huge files that requires a lot of RAM. Splitting the files enables us to load one fold at a time, thereby, reducing the load on memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "#setting up path to dataset\n",
    "user_json = os.path.join(\"..\",\"data\",\"user.json\")\n",
    "user_data_dir = os.path.join(\"..\",\"data\",\"user\")\n",
    "review_json = os.path.join(\"..\",\"data\",\"review.json\")\n",
    "review_data_dir = os.path.join(\"..\",\"data\",\"review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting review json to chunks of csv files...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "\n",
    "def convert_review_to_csv(json_file_path, out_dir, lines_per_file):\n",
    "    columns = ['user_id','business_id','text','stars','useful','funny','cool']\n",
    "    json_file = open(json_file_path, 'r', encoding=\"utf8\")\n",
    "    count = 0\n",
    "    fname = 1\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    output = csv.writer(open(os.path.join(out_dir,str(fname)+\".csv\"), \"w+\", encoding=\"utf8\", newline=''))\n",
    "    output.writerow(columns)\n",
    "    for line in json_file:\n",
    "        count += 1\n",
    "        j = json.loads(line)\n",
    "        j['text'] = j['text'].replace(\"\\n\",\" \")\n",
    "        csv_arr = list()\n",
    "        for column in columns:\n",
    "            if column in j:\n",
    "                csv_arr.append(j[column])\n",
    "        output.writerow(csv_arr)\n",
    "        if count == lines_per_file:\n",
    "            count = 0\n",
    "            fname += 1\n",
    "            output = csv.writer(open(os.path.join(out_dir,str(fname)+\".csv\"), \"w+\", encoding=\"utf8\", newline=''))\n",
    "    json_file.close()\n",
    "    \n",
    "print(\"Converting review json to chunks of csv files...\")\n",
    "review_json = os.path.join(\"..\",\"data\",\"review.json\")\n",
    "review_csv_dir = os.path.join(\"..\",\"data\",\"review_csv\")\n",
    "convert_review_to_csv(json_file_path=review_json, out_dir=review_csv_dir, lines_per_file=50000)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting user json to chunks of csv files...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "\n",
    "def convert_user_to_csv(json_file_path, out_dir, lines_per_file):\n",
    "    columns = ['user_id', 'name', 'review_count', 'yelping_since', 'useful', 'funny', 'cool', \n",
    "               'fans', 'elite', 'average_stars', 'compliment_hot', 'compliment_more', 'compliment_profile', \n",
    "               'compliment_cute', 'compliment_list', 'compliment_note', 'compliment_plain', 'compliment_cool', \n",
    "               'compliment_funny', 'compliment_writer', 'compliment_photos']\n",
    "    json_file = open(json_file_path, 'r', encoding=\"utf8\" )\n",
    "    count = 0\n",
    "    fname = 1\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    output = csv.writer(open(os.path.join(out_dir,str(fname)+\".csv\"), \"w+\", encoding=\"utf8\", newline=''))\n",
    "    output.writerow(columns)\n",
    "    for line in json_file:\n",
    "        count += 1\n",
    "        j = json.loads(line)\n",
    "        csv_arr = list()\n",
    "        for column in columns:\n",
    "            if column in j:\n",
    "                csv_arr.append(j[column])\n",
    "        output.writerow(csv_arr)\n",
    "        if count == lines_per_file:\n",
    "            count = 0\n",
    "            fname += 1\n",
    "            output = csv.writer(open(os.path.join(out_dir,str(fname)+\".csv\"), \"w+\", encoding=\"utf8\", newline=''))\n",
    "    json_file.close()\n",
    "    \n",
    "print(\"Converting user json to chunks of csv files...\")\n",
    "user_json = os.path.join(\"..\",\"data\",\"user.json\")\n",
    "user_csv_dir = os.path.join(\"..\",\"data\",\"user_csv\")\n",
    "convert_user_to_csv(json_file_path=user_json, out_dir=user_csv_dir, lines_per_file=50000)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting business json to chunks of csv files...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import json\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "def convert_business_to_csv(json_file_path, out_dir, lines_per_file):\n",
    "    json_file = open(json_file_path, 'r', encoding=\"utf8\")\n",
    "    business = list()    \n",
    "    count = 0\n",
    "    for line in json_file:\n",
    "        count += 1\n",
    "        j = json.loads(line.rstrip())\n",
    "        business.append(j['business_id'])\n",
    "    json_file.close()\n",
    "    \n",
    "    output = csv.writer(open(os.path.join(out_dir,\"business_id.csv\"), \"w+\", encoding=\"utf8\", newline=''))\n",
    "    for b in range(len(business)):\n",
    "        output.writerow([str(business[b]), b])\n",
    "\n",
    "#     columns = ['business_id','name','neighborhood','address','city','state','postal_code','latitude','longitude','stars','categories']\n",
    "    columns = ['business_id','latitude','longitude','stars','categories']\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    output = csv.writer(open(os.path.join(out_dir,\"business.csv\"), \"w+\", encoding=\"utf8\", newline=''))\n",
    "    output.writerow(columns)\n",
    "    json_file = open(json_file_path, 'r', encoding=\"utf8\")\n",
    "    count = 0\n",
    "    l = list()\n",
    "    for line in json_file:\n",
    "        count+=1\n",
    "        csv_arr = list()\n",
    "        j = json.loads(line)\n",
    "        for column in columns:\n",
    "            if column == 'business_id':\n",
    "                csv_arr.append(count-1)\n",
    "            else:\n",
    "                column_hierarchy = column.split(\".\")\n",
    "                json_obj = j\n",
    "                is_there = True\n",
    "                for c in column_hierarchy:\n",
    "                    if c in json_obj:\n",
    "                        json_obj = json_obj[c]\n",
    "                    else:\n",
    "                        is_there = False\n",
    "                        break\n",
    "                if is_there:\n",
    "                    if isinstance(json_obj,list):\n",
    "                        categories = \"\"\n",
    "                        for obj in json_obj:\n",
    "                            categories += \", \"+obj;\n",
    "                        if categories[2:] == '':\n",
    "                            csv_arr.append(numpy.nan)\n",
    "                        else:\n",
    "                            csv_arr.append(categories[2:])\n",
    "                    else:\n",
    "                        if json_obj == '':\n",
    "                            csv_arr.append(numpy.nan)\n",
    "                        else:\n",
    "                            csv_arr.append(json_obj)\n",
    "        l.append(csv_arr)\n",
    "#         output.writerow(csv_arr)\n",
    "    l = pandas.DataFrame(l)\n",
    "    l = l.dropna(axis=0, how='any')\n",
    "    for row in l.values:\n",
    "       # print(row)\n",
    "        output.writerow(row)\n",
    "    json_file.close()\n",
    "    \n",
    "    \n",
    "print(\"Converting business json to chunks of csv files...\")\n",
    "biz_json = os.path.join(\"..\",\"data\",\"business.json\")\n",
    "biz_csv_dir = os.path.join(\"..\",\"data\",\"biz_csv\")\n",
    "convert_business_to_csv(json_file_path=biz_json, out_dir=biz_csv_dir, lines_per_file=50000)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking business and its attributes into a separate file...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import json\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "def convert_business_to_csv(json_file_path, out_dir, lines_per_file):\n",
    "    json_file = open(json_file_path, 'r', encoding=\"utf8\")\n",
    "    business = list()    \n",
    "    count = 0\n",
    "    for line in json_file:\n",
    "        count += 1\n",
    "        j = json.loads(line.rstrip())\n",
    "        business.append(j['business_id'])\n",
    "    json_file.close()\n",
    "    \n",
    "    output = csv.writer(open(os.path.join(out_dir,\"business_id.csv\"), \"w+\", encoding=\"utf8\", newline=''))\n",
    "    for b in range(len(business)):\n",
    "        output.writerow([str(business[b]), b])\n",
    "\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    output = csv.writer(open(os.path.join(out_dir,\"business_attributes.csv\"), \"w+\", encoding=\"utf8\", newline=''))\n",
    "    output.writerow([\"business_id\",\"attributes\"])\n",
    "    json_file = open(json_file_path, 'r', encoding=\"utf8\")\n",
    "    count = 0\n",
    "    l = list()\n",
    "    for line in json_file:\n",
    "        count+=1\n",
    "        csv_arr = list()\n",
    "        j = json.loads(line)\n",
    "        csv_arr.append(count-1)\n",
    "        if \"attributes\" in j:\n",
    "            csv_arr.append(j[\"attributes\"])\n",
    "        if(not str(j[\"attributes\"]) == \"{}\"):\n",
    "            l.append(csv_arr)\n",
    "    l = pandas.DataFrame(l)\n",
    "    l = l.dropna(axis=0, how='any')\n",
    "    for row in l.values:\n",
    "        output.writerow(row)\n",
    "    json_file.close()\n",
    "    \n",
    "    \n",
    "print(\"Taking business and its attributes into a separate file...\")\n",
    "biz_json = os.path.join(\"..\",\"data\",\"business.json\")\n",
    "biz_csv_dir = os.path.join(\"..\",\"data\",\"biz_csv\")\n",
    "convert_business_to_csv(json_file_path=biz_json, out_dir=biz_csv_dir, lines_per_file=50000)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting check-in json to csv....\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "def check_in_pre_processing(file_path, out_dir):\n",
    "    check_in_file = open(file_path,\"r\", encoding=\"utf8\")\n",
    "    day_map = {\n",
    "        \"Sunday\" : 1,\n",
    "        \"Monday\" : 2,\n",
    "        \"Tuesday\" : 3,\n",
    "        \"Wednesday\" : 4,\n",
    "        \"Thursday\" : 5,\n",
    "        \"Friday\" : 6,\n",
    "        \"Saturday\" : 7\n",
    "    }\n",
    "    time_map = {\n",
    "        \"0:00\" : 0,\n",
    "        \"1:00\" : 1,\n",
    "        \"2:00\" : 2,\n",
    "        \"3:00\" : 3,\n",
    "        \"4:00\" : 4,\n",
    "        \"5:00\" : 5,\n",
    "        \"6:00\" : 6,\n",
    "        \"7:00\" : 7,\n",
    "        \"8:00\" : 8,\n",
    "        \"9:00\" : 9,\n",
    "        \"10:00\" : 10,\n",
    "        \"11:00\" : 11,\n",
    "        \"12:00\" : 12,\n",
    "        \"13:00\" : 13,\n",
    "        \"14:00\" : 14,\n",
    "        \"15:00\" : 15,\n",
    "        \"16:00\" : 16,\n",
    "        \"17:00\" : 17,\n",
    "        \"18:00\" : 18,\n",
    "        \"19:00\" : 19,\n",
    "        \"20:00\" : 20,\n",
    "        \"21:00\" : 21,\n",
    "        \"22:00\" : 22,\n",
    "        \"23:00\" : 23\n",
    "    }\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    output = csv.writer(open(os.path.join(out_dir,\"checkin.csv\"), \"w+\", encoding=\"utf8\", newline=''))\n",
    "    output.writerow([\"business_id\",\"day_of_the_week\",\"time_of_the_day\",\"no_of_checkins\"])\n",
    "    bids = set()\n",
    "    for line in check_in_file:\n",
    "        j = json.loads(line)\n",
    "        bid = j['business_id']\n",
    "        bids.add(bid)\n",
    "    bids = list(bids)\n",
    "    bid_map = dict()\n",
    "    i = 0\n",
    "    with open(os.path.join(\"..\",\"data\",\"biz_csv\",\"business_id.csv\"), \"r\", encoding=\"utf8\") as bid_map_file:\n",
    "        for line in bid_map_file:\n",
    "            line = line.strip().split(\",\")\n",
    "            if len(line) != 2:\n",
    "                continue;\n",
    "#             print(line)\n",
    "            bid_map[line[0]] = line[1] \n",
    "    \n",
    "    check_in_file.seek(0)\n",
    "    for line in check_in_file:\n",
    "        j = json.loads(line)\n",
    "        bid = j['business_id']\n",
    "        for day in j['time']:\n",
    "            for time in j['time'][day]:\n",
    "                csv_arr = [bid_map[bid],day_map[day],time_map[time],j['time'][day][time]]\n",
    "                output.writerow(csv_arr)\n",
    "    check_in_file.close()    \n",
    "\n",
    "print(\"Converting check-in json to csv....\")\n",
    "checkin_input = os.path.join(\"..\",\"data\",\"checkin.json\")\n",
    "checkin_csv_dir = os.path.join(\"..\",\"data\",\"checkin_csv\")\n",
    "check_in_pre_processing(checkin_input,checkin_csv_dir)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utility to get a summary of the dataset...\n",
    "import os\n",
    "import json \n",
    "\n",
    "def get_summary(file_path):\n",
    "    file = open(file_path, 'r', encoding=\"utf-8\")\n",
    "    line = json.loads(file.readline())\n",
    "    attributes = list(line.keys())\n",
    "#     count = 1\n",
    "#     for line in file:\n",
    "#         print(line)\n",
    "#         count+=1\n",
    "#     file.close()\n",
    "    count = len(attributes)\n",
    "\n",
    "    return (count,attributes)\n",
    "\n",
    "summary = get_summary(os.path.join(\"..\",\"data\",\"review.json\"))\n",
    "print(\"Summary of review.json :\")\n",
    "print(\"Total number of records: \"+str(summary[0]))\n",
    "print(\"Attributes: \",summary[1])\n",
    "\n",
    "summary = get_summary(os.path.join(\"..\",\"data\",\"user.json\"))\n",
    "print(\"\\nSummary of user.json :\")\n",
    "print(\"Total number of records: \"+str(summary[0]))\n",
    "print(\"Attributes: \",summary[1])\n",
    "\n",
    "summary = get_summary(os.path.join(\"..\",\"data\",\"business.json\"))\n",
    "print(\"\\nSummary of business.json :\")\n",
    "print(\"Total number of records: \"+str(summary[0]))\n",
    "print(\"Attributes: \",summary[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Data Distribution Charts:\n",
    "\n",
    "- Number of reviews per year (trend chart)\n",
    "- Number of users who joined yelp per year\n",
    "- Number of reviews per possible rating value\n",
    "- Number of useful, cool and funny reviews per business (there are nearly 156639 businesses. Takes a lot of time to render graph)\n",
    "- Number of reviews per business (there are nearly 156639 businesses. Takes a lot of time to render graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Rating vs Number of reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plotter\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def reviews_vs_rating(file_path):\n",
    "    rating = [0,0,0,0,0,0]\n",
    "    file = open(file_path, 'r', encoding='utf-8')\n",
    "    for line in file:\n",
    "        j = json.loads(line)\n",
    "        rating[j[\"stars\"]]+=1\n",
    "    file.close()\n",
    "    return rating\n",
    "\n",
    "ratings = reviews_vs_rating(os.path.join(\"..\",\"data\",\"review.json\"))\n",
    "plotter.bar([1,2,3,4,5],ratings[1:])\n",
    "plotter.xlabel('Rating')\n",
    "plotter.ylabel('No. of Reviews')\n",
    "plotter.title('Rating vs No. of Reviews')\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plotter\n",
    "%matplotlib inline\n",
    "def reviews_vs_date(file_path):\n",
    "    dates = dict()\n",
    "    file = open(file_path, 'r', encoding='utf-8')\n",
    "    for line in file:\n",
    "        j = json.loads(line)\n",
    "        d = j[\"date\"][0:4]\n",
    "        if d in dates:\n",
    "            dates[d] += 1\n",
    "        else:\n",
    "            dates[d] = 1\n",
    "    file.close()\n",
    "    return dates\n",
    "\n",
    "dates = reviews_vs_date(os.path.join(\"..\",\"data\",\"review.json\"))\n",
    "\n",
    "ax = plotter.subplot(111)\n",
    "ax.bar(range(len(dates.keys())),list(dates.values()))\n",
    "ax.set_xticklabels(dates.keys())\n",
    "plotter.xlabel('Year')\n",
    "plotter.ylabel('No. of Reviews')\n",
    "plotter.title('Year vs No. of Reviews')\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plotter\n",
    "%matplotlib inline\n",
    "def reviews_vs_date(file_path):\n",
    "    dates = dict()\n",
    "    file = open(file_path, 'r', encoding=\"utf8\")\n",
    "    for line in file:\n",
    "        j = json.loads(line)\n",
    "        d = j[\"yelping_since\"][0:4]\n",
    "        if d in dates:\n",
    "            dates[d] += 1\n",
    "        else:\n",
    "            dates[d] = 1\n",
    "    file.close()\n",
    "    return dates\n",
    "\n",
    "dates = reviews_vs_date(os.path.join(\"..\",\"data\",\"user.json\"))\n",
    "\n",
    "ax = plotter.subplot(111)\n",
    "ax.bar(range(len(dates.keys())),list(dates.values()))\n",
    "ax.set_xticklabels(dates.keys())\n",
    "plotter.xlabel('Year')\n",
    "plotter.ylabel('No. of users who joined Yelp')\n",
    "plotter.title('Year vs No. of users who joined Yelp')\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plotter\n",
    "%matplotlib inline\n",
    "def checkin_vs_date(file_path):\n",
    "    dates = dict()\n",
    "    with open(file_path,\"r\", encoding=\"utf8\") as file:\n",
    "        for i in range(4):\n",
    "            line = file.readline()\n",
    "            j = json.loads(line)\n",
    "            check_ins = list()\n",
    "            days = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "            for day in days:\n",
    "                if day in j['time']:\n",
    "                    total_check_in = 0\n",
    "                    for time in j['time'][day].keys():\n",
    "                        total_check_in += j['time'][day][time]\n",
    "                    check_ins.append(total_check_in)\n",
    "                else:\n",
    "                    check_ins.append(0)\n",
    "            dates[j['business_id']] = check_ins\n",
    "            \n",
    "    return dates\n",
    "\n",
    "dates = checkin_vs_date(os.path.join(\"..\",\"data\",\"checkin.json\"))\n",
    "print(dates)\n",
    "\n",
    "for business in dates.keys():\n",
    "    plotter.bar([1,2,3,4,5,6,7],dates[business])\n",
    "    plotter.xlabel('Day of the week')\n",
    "    plotter.ylabel('No. of Check-ins')\n",
    "    plotter.title('Day of the week vs No. of Check-ins')\n",
    "    plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[124296      5     21]\n",
      " [124296      5      1]\n",
      " [124296      5      4]\n",
      " ..., \n",
      " [143173      4      0]\n",
      " [143173      4      1]\n",
      " [143173      4      8]]\n",
      "[4 1 1 ..., 3 4 1]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas\n",
    "\n",
    "checkins = pandas.read_csv(os.path.join(\"..\",\"data\",\"checkin_csv\",\"checkin.csv\"))\n",
    "checkin_vals = checkins['no_of_checkins']\n",
    "checkins = checkins.drop(['no_of_checkins'],axis=1)\n",
    "print(checkins.values)\n",
    "print(checkin_vals.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn import linear_model\n",
    "\n",
    "#reg = linear_model.LinearRegression()\n",
    "#reg.fit(checkins.values, checkin_vals.values)\n",
    "#reg.predict([[21589,6,13]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "ann = MLPRegressor(solver='sgd',hidden_layer_sizes=(2), random_state=1, max_iter=100)\n",
    "ann.fit(checkins.values, checkin_vals.values)\n",
    "ann.predict([[21589,6,13]])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ann.predict([[2573,3,23]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
